# oasis_scraper.py
import requests
from bs4 import BeautifulSoup
import json
import time
import os

# ===============================
# ğŸŒ¿ ì•½ì¬ ë°ì´í„° ìˆ˜ì§‘ (Herb)
# ===============================
def scrape_herbs(start=1, end=30):
    """
    OASIS í•œì•½ì¬ ìƒì„¸ í˜ì´ì§€ (monoDetailView_M01.jsp)ì—ì„œ ì•½ì¬ ë°ì´í„° ìˆ˜ì§‘
    """
    base_url = "https://oasis.kiom.re.kr/oasis/herb/monoDetailView_M01.jsp?idx="
    all_herbs = []

    for herb_id in range(start, end + 1):
        url = base_url + str(herb_id)
        print(f"[HERB] {url}")

        try:
            res = requests.get(url, timeout=10)
            if res.status_code != 200:
                print(f"âš ï¸ ì ‘ê·¼ ë¶ˆê°€ ({res.status_code})")
                continue

            soup = BeautifulSoup(res.text, "html.parser")

            # âœ… ì œëª© (ì•½ì¬ëª…) â€” .cont_view ë‚´ë¶€ì˜ h2ë§Œ ì„ íƒ
            title_tag = soup.select_one(".cont_view h2")
            title = title_tag.get_text(strip=True) if title_tag else f"Unknown_{herb_id}"

            herb_data = {"id": herb_id, "name": title, "url": url}

            # âœ… í‘œ(table) ì•ˆì˜ ë°ì´í„° ì¶”ì¶œ
            for table in soup.select(".cont_view table"):
                for tr in table.select("tr"):
                    ths = tr.find_all("th")
                    tds = tr.find_all("td")
                    for th, td in zip(ths, tds):
                        key = th.get_text(strip=True)
                        val = td.get_text(strip=True)
                        herb_data[key] = val

            # âœ… ë³¸ë¬¸ ë‚´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì €ì¥
            desc_blocks = soup.select(".cont_view, .view_con, .view_cont")
            if desc_blocks:
                herb_data["ì„¤ëª…"] = " ".join([d.get_text(" ", strip=True) for d in desc_blocks])

            if len(herb_data) > 2:
                all_herbs.append(herb_data)
                print("âœ… ìˆ˜ì§‘:", title)
            else:
                print("âš ï¸ ë°ì´í„° ì—†ìŒ:", title)

            time.sleep(0.2)

        except Exception as e:
            print("âŒ Error:", e)

    os.makedirs("../raw", exist_ok=True)
    with open("../raw/oasis_herbs_raw.json", "w", encoding="utf-8") as f:
        json.dump(all_herbs, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved {len(all_herbs)} herb entries")
    return all_herbs


# ===============================
# ğŸ’Š ì²˜ë°© ë°ì´í„° ìˆ˜ì§‘ (Prescription)
# ===============================
def scrape_prescriptions(start=1, end=30):
    """
    OASIS ì²˜ë°© ìƒì„¸ í˜ì´ì§€ (view01~view03)ë¥¼ ìˆœíšŒí•˜ë©°
    ì²˜ë°©ëª… / êµ¬ì„± / íš¨ëŠ¥ ë° ì£¼ì¹˜ ë°ì´í„°ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ìˆ˜ì§‘
    """
    base_url = "https://oasis.kiom.re.kr/oasis/pres/prdetailView.jsp"
    all_pres = []

    for idx in range(start, end + 1):
        print(f"\n[PRES] idx={idx}")
        pres_data = {"id": idx, "url": f"{base_url}?idx={idx}"}

        try:
            # -----------------------------
            # ğŸ”¹ view01 : ì²˜ë°©ëª… (ê¸°ë³¸ì •ë³´)
            # -----------------------------
            url1 = f"{base_url}?idx={idx}#view01"
            res1 = requests.get(url1, timeout=10)
            if res1.status_code == 200:
                soup1 = BeautifulSoup(res1.text, "html.parser")

                # âœ… ì œëª©: .cont_view ë‚´ë¶€ì˜ h2ë§Œ ì„ íƒ
                title_tag = soup1.select_one(".cont_view h2")
                if title_tag:
                    title_text = title_tag.get_text(" ", strip=True)
                    pres_data["ì²˜ë°©ëª…"] = title_text.split(",")[0].strip()
                else:
                    print("âš ï¸ ì œëª© íƒœê·¸ ì—†ìŒ")

                # âœ… í‘œ íŒŒì‹± (ë³¸ë¬¸ ë‚´ë¶€)
                for tr in soup1.select(".cont_view table tr"):
                    ths = tr.find_all("th")
                    tds = tr.find_all("td")
                    for th, td in zip(ths, tds):
                        key = th.get_text(strip=True)
                        val = td.get_text(strip=True)
                        if key and val:
                            pres_data[key] = val

            # -----------------------------
            # ğŸ”¹ view02 : êµ¬ì„± (ì‹œëŒ€ë³„ êµ¬ì„± í¬í•¨)
            # -----------------------------
            url2 = f"{base_url}?idx={idx}#view02"
            res2 = requests.get(url2, timeout=10)
            if res2.status_code == 200:
                soup2 = BeautifulSoup(res2.text, "html.parser")
                composition_texts = []
                for block in soup2.select(".cont_view, .view_con, .view_cont"):
                    composition_texts.append(block.get_text(" ", strip=True))
                if composition_texts:
                    pres_data["êµ¬ì„±"] = " ".join(composition_texts)

            # -----------------------------
            # ğŸ”¹ view03 : íš¨ëŠ¥ ë° ì£¼ì¹˜
            # -----------------------------
            url3 = f"{base_url}?idx={idx}#view03"
            res3 = requests.get(url3, timeout=10)
            if res3.status_code == 200:
                soup3 = BeautifulSoup(res3.text, "html.parser")

                efficacy = {}
                for tr in soup3.select(".cont_view table tr"):
                    tds = tr.find_all("td")
                    ths = tr.find_all("th")
                    for th, td in zip(ths, tds):
                        key = th.get_text(strip=True)
                        val = td.get_text(strip=True)
                        if key and val:
                            efficacy[key] = val

                if efficacy:
                    pres_data["íš¨ëŠ¥ ë° ì£¼ì¹˜"] = efficacy

                extra_blocks = soup3.select(".cont_view, .view_con, .view_cont")
                texts = [b.get_text(" ", strip=True) for b in extra_blocks]
                if texts:
                    pres_data["íš¨ëŠ¥ì„¤ëª…"] = " ".join(texts)

            # -----------------------------
            # âœ… ì €ì¥ ì¡°ê±´
            # -----------------------------
            if "ì²˜ë°©ëª…" in pres_data and pres_data["ì²˜ë°©ëª…"] != "ì£¼ ë©”ë‰´":
                all_pres.append(pres_data)
                print(f"âœ… ìˆ˜ì§‘: {pres_data['ì²˜ë°©ëª…']}")
            else:
                print("âš ï¸ ì²˜ë°©ëª… ì—†ìŒ ë˜ëŠ” ë¬´íš¨, skip")

            time.sleep(0.3)

        except Exception as e:
            print("âŒ Error:", e)

    os.makedirs("../raw", exist_ok=True)
    with open("../raw/oasis_pres_raw.json", "w", encoding="utf-8") as f:
        json.dump(all_pres, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Saved {len(all_pres)} prescription entries")
    return all_pres


# ===============================
# ğŸš€ ë©”ì¸ ì‹¤í–‰
# ===============================
if __name__ == "__main__":
    print("ğŸ©º OASIS Data Scraper Starting...")

    herbs = scrape_herbs(start=1, end=30)
    pres = scrape_prescriptions(start=1, end=30)

    print("âœ… All done!")
